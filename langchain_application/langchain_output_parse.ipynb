{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUwEabvsBg1E8rt2esJeIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelam98/LangChain_Fundamentals/blob/main/langchain_application/langchain_output_parse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LangChain Explained In 15 Minutes - A MUST Learn For Python Programmers](https://www.youtube.com/watch?v=mrjq3lFz23s)"
      ],
      "metadata": {
        "id": "BPIQsDKHRj5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "g_M7amg78Atk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# packages:\n",
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith langchain_google_genai langchain   python-dotenv"
      ],
      "metadata": {
        "id": "fm15AYByRjW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuPB4-ejRFVS"
      },
      "outputs": [],
      "source": [
        "# API keys set up\n",
        "# Configure environment to connect to LangSmith.\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain_applications\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "Cymoq5hfUOJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    max_retries=2,\n",
        "    api_key=gemini_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "JyIYeIRtUWoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Output parser](https://python.langchain.com/docs/concepts/output_parsers/)\n",
        "\n",
        "**Output parser** is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs."
      ],
      "metadata": {
        "id": "3yEUKakUoeC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`ChatPromptTemplate.from_messages()`**\n",
        "\n",
        "**`llm.predict_messages()`**"
      ],
      "metadata": {
        "id": "G2UEsd0Lk_jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Language Translation Agent with LangChain\n",
        "\n",
        "**`ChatPromptTemplate.from_messages()`**\n",
        "\n",
        "**`llm.predict_messages()`**"
      ],
      "metadata": {
        "id": "FhkR17AKf5SI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Language Translation Agent with LangChain\n",
        "\n",
        "# Import Required Libraries\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Initialize Language Model\n",
        "chat_model = llm\n",
        "\n",
        "# Define Prompt Templates\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "# Create a prompt template\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", template),\n",
        "        (\"human\", human_template)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Format the messages using the template\n",
        "messages = chat_prompt.format_messages(\n",
        "    input_language=\"English\",\n",
        "    output_language=\"French\",\n",
        "    text=\"I am an expert in AI Agents.\"\n",
        ")\n",
        "\n",
        "# chat_model = llm\n",
        "# Use the chat model to predict the response (notice)\n",
        "result = chat_model.predict_messages(messages)\n",
        "\n",
        "# Output the translated content\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W80pCtefC8q",
        "outputId": "544df3c0-8e80-44a6-8a11-10c67cb34948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e8e4d9dc0a65>:25: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chat_model.predict_messages(messages)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je suis expert en agents IA.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Assuming llm is already defined\n",
        "chat_model = llm\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that solves math problems and shows your work.\n",
        "Output each step and then return the answer in the following format:\n",
        "answer = <answer here>.\n",
        "Make sure to output the answer in all lowercase with exactly one space and one equal sign following it'answer'.\n",
        "\"\"\"\n",
        "\n",
        "human_template = \"{problem}\"\n",
        "\n",
        "# Correctly initializing the ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", template),\n",
        "        (\"human\", human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Formatting the messages\n",
        "messages = chat_prompt.format_messages(problem=\"2x^2 - 5x + 3 = 0\")\n",
        "\n",
        "# Getting the response\n",
        "result = llm.predict_messages(messages)\n",
        "\n",
        "# Direct parsing of the content\n",
        "response_content = result.content\n",
        "\n",
        "# Print the response for debugging\n",
        "print(\"Response from LLM:\", response_content)\n",
        "\n",
        "# Parsing steps (optional, based on how the output is structured)\n",
        "# Here, parsing should handle splitting steps and the final answer\n",
        "if \"answer =\" in response_content:\n",
        "    steps, answer = response_content.rsplit(\"answer =\", 1)\n",
        "    print(\"Steps:\\n\", steps.strip())\n",
        "    print(\"Answer:\", answer.strip())\n",
        "else:\n",
        "    print(\"Could not parse the answer.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k60GKFWQk-xa",
        "outputId": "830f81a1-0bfa-4892-b7e5-0172d11c1dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from LLM: Here's how to solve the quadratic equation 2x² - 5x + 3 = 0:\n",
            "\n",
            "**1. Factoring**\n",
            "\n",
            "We look for two numbers that add up to -5 (the coefficient of x) and multiply to 6 (the product of the coefficient of x² and the constant term). Those numbers are -2 and -3.  We can rewrite the equation as:\n",
            "\n",
            "2x² - 2x - 3x + 3 = 0\n",
            "\n",
            "**2. Grouping**\n",
            "\n",
            "Now we group the terms and factor:\n",
            "\n",
            "2x(x - 1) - 3(x - 1) = 0\n",
            "\n",
            "**3. Factoring out the common term**\n",
            "\n",
            "(x - 1)(2x - 3) = 0\n",
            "\n",
            "**4. Solving for x**\n",
            "\n",
            "This equation is true if either (x - 1) = 0 or (2x - 3) = 0.  Solving each gives us:\n",
            "\n",
            "x - 1 = 0  =>  x = 1\n",
            "\n",
            "2x - 3 = 0  =>  2x = 3  =>  x = 3/2\n",
            "\n",
            "**Answer = 1 and 3/2**\n",
            "\n",
            "Could not parse the answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Output Parse**"
      ],
      "metadata": {
        "id": "Y2QrIouayHj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Parse\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import output_parser, BaseOutputParser\n",
        "\n",
        "class AnswerOutputParser(BaseOutputParser):\n",
        "  def parse(self, text: str):\n",
        "    \"\"\"Parse the output of an LLM call. \"\"\"\n",
        "    return text.strip().split(\"answer = \")\n",
        "\n",
        "\n",
        "chat_model = llm\n",
        "\n",
        "template = \"\"\" You are a helpful assistant that solves math problems and show your work.\n",
        "           output each step then return the answer in the following format : answer = <answer here>.\n",
        "           Make sure to output answer in all lowercase and to have exactly one space and one equal sign follow it.\n",
        "           \"\"\"\n",
        "\n",
        "human_template = \"{problem}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", template),\n",
        "        (\"human\", human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_prompt.format_messages(problem=\"2x2 - 5x + 3 = 0\")\n",
        "\n",
        "result = llm.predict_messages(messages)\n",
        "\n",
        "parsed = AnswerOutputParser().parse(result.content)\n",
        "steps, answer = parsed\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm_3UeCUnQEi",
        "outputId": "030fb495-02c0-47d9-f9c5-7e229dd3c152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 1, x = 3/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comma Seprated List Output Parse in Chain\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import output_parser, BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "  def parse(self, text: str):\n",
        "    return text.strip().split(\", \")\n",
        "\n",
        "\n",
        "chat_model = llm\n",
        "\n",
        "template = \"\"\" You are a helpful assistant who gerates comma separated lists.\n",
        "           A user will pass in a category, and you should generate 5 objects in that category in a comma seprated List.\n",
        "           ONLY return a comma separated list, nothing more.\n",
        "           \"\"\"\n",
        "\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", template),\n",
        "        (\"human\", human_template),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# | use to combine all of them into one object\n",
        "chain = chat_prompt | llm | CommaSeparatedListOutputParser()\n",
        "\n",
        "result = chain.invoke({\"text\": \"colors\"})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcgiTVVnyzL_",
        "outputId": "cd865b0e-854c-48be-da9c-4242e7cbea9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'green', 'blue', 'yellow', 'orange']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uN9BbDUh2CJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}